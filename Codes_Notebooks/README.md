## First notebook : Project guidelines
On that part, we trained models using our raw data, without cleaning them
## Second notebook : Different tokenizers
On that notebook, we tried to tokenize using several ways, particularly with the following libraries : regular expression (re) and spacy tokenizer
## Third notebook : Text classification using Feature Extractions
On that last part, we decided to create a new DataFrame containing new features about the properties of the sentences. Once cleaned, we used the ntlk library to clean the data before training them in our models.
